{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenForecast - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwoebbxWgzEQ",
    "tags": []
   },
   "source": [
    "# Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1654967842227,
     "user": {
      "displayName": "Matt Yeung",
      "userId": "15328585613911183133"
     },
     "user_tz": -600
    },
    "id": "f9eM-2oIMARa"
   },
   "outputs": [],
   "source": [
    "# !pip install -Uqq fastai\n",
    "# !pip install -Uqq xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "import re\n",
    "import json\n",
    "np.random.seed(2)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import xgboost\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# autoreload changed modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "# plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "plt.rcParams[\"figure.figsize\"] = (32,12)\n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = 'dataset8'  # files will be {DATASET_NAME}_{REGION}.csv eg dataset8_NSW1.csv\n",
    "DATASET_FOLDER = Path('../data')\n",
    "REGIONIDS = ['NSW1', 'QLD1', 'SA1', 'TAS1', 'VIC1']\n",
    "FORECAST_TIMES = list(range(2,24,2)) + list(range(24, 24*7 + 4, 4))\n",
    "HOLDOUT_SET_START = '2022-09-01'\n",
    "p_min, p_max = -400, 1000 # values to clip price columns to when decoding\n",
    "\n",
    "\n",
    "# make sure there is a models subdirectory\n",
    "if not os.path.exists('models'): os.mkdir('models')\n",
    "\n",
    "# print ALL columns from just two rows, displays all even when pandas cuts everything off.\n",
    "def print_item(df, index=0, offset=1):\n",
    "    if isinstance(index, str):\n",
    "        # if passing in a string, it's probably going for loc instead of iloc, convert.\n",
    "        index = df.index.get_loc(index)\n",
    "    cells = list(zip([x for x in df.columns],[x for x in df.iloc[index]],[x for x in df.iloc[index+offset]]))\n",
    "    [print(f'{a[0]:32}{a[1]:<32}{a[2]:<32}') for a in cells]\n",
    "\n",
    "def check_nas(df):\n",
    "    return df.loc[df.isnull().any(axis=1)]\n",
    "\n",
    "def pkl(thing, filename):\n",
    "    pickle.dump( thing, open( \"filename\", \"wb\" ) )\n",
    "\n",
    "def upkl(filename):\n",
    "    with open('filename', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 8870,
     "status": "ok",
     "timestamp": 1654968352313,
     "user": {
      "displayName": "Matt Yeung",
      "userId": "15328585613911183133"
     },
     "user_tz": -600
    },
    "id": "ba2l01SFTxPB",
    "outputId": "b8bd439f-e5e3-4281-c912-bc1fb0cc09e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### READ DATASET FILE\n",
    "#####################\n",
    "\n",
    "''' ignored_columns() returns the columns that should be ignored for a given target forecast \n",
    "Including: \n",
    "- forcasts more than 24 away from the target time\n",
    "- lags of the other variable (ie greenness lags if we're predicting price) \n",
    "'''\n",
    "def ignored_columns(columns, target, lag):\n",
    "    ignored = []\n",
    "    for col in columns:\n",
    "        if \"_Tp\" not in col: continue  # only concerned with forecst cols '_Tp'\n",
    "        if \"Predis\" in col: continue  # want predispatch columns always, don't ignore\n",
    "        time = int(col.split(\"_Tp\")[1]) \n",
    "        if time > lag + 24: ignored.append(col)\n",
    "        if time < lag - 24: ignored.append(col)\n",
    "        \n",
    "    # remove greenness-specific columns from price\n",
    "    if 'Price' in target:\n",
    "        ignored = ignored + [col for col in columns if '_Greenness_Tm' in col]\n",
    "        ignored = ignored + [col for col in columns if '_IC_Fossil_In' in col or '_IC_Green_In' in col]\n",
    "        \n",
    "    else: # remove price-specific columns from greenness\n",
    "        ignored = ignored + [x for x in columns if re.match('.*Price_Tm\\d+\\Z', x)] # ignore _Price_Tm20 but keep _Price_Tm30d\n",
    "        \n",
    "    return ignored\n",
    "\n",
    "def read_dataset(region='VIC1', target=\"VIC1_Price_Tp84\", lag=84, switch='full_training', validation_set=0):\n",
    "    global df, y_names, continuous, categorical  # declaring global so can inspect later. Don't edit \n",
    "    global dataset  # data as read from the .csv file, before any time-specific changes made.  declaring global so can inspect later and also to keep it static \n",
    "    \n",
    "    # don't read if we've already got the right dataset in memory\n",
    "    if 'dataset' in globals() and f\"{region}_Price\" in dataset.columns:    \n",
    "        print(f\"Already got {region} datafile in memory\")\n",
    "    else:\n",
    "        datafile = DATASET_FOLDER / f'{DATASET_NAME}_{region}.csv'\n",
    "        print(f\"Datafile = {datafile}\")\n",
    "\n",
    "        # we want to read as float32 not double. First, sample 1000 rows of data to determine dtypes.\n",
    "        df_test = pd.read_csv(datafile, nrows=1000)\n",
    "        float_cols = [c for c in df_test if str(df_test[c].dtype) in [\"float64\", \"int64\", \"bool\"]]\n",
    "        float32_cols = {c: np.float32 for c in float_cols}\n",
    "        \n",
    "        dataset = pd.read_csv(datafile, parse_dates=[0], index_col=0, engine='c', dtype=float32_cols)\n",
    "\n",
    "        assert dataset.index.name == \"Date\"\n",
    "        \n",
    "        # Transform price columns\n",
    "        for col in dataset.columns:\n",
    "            if 'Price' in col:\n",
    "                dataset[col] = dataset[col].clip(lower=p_min, upper=p_max)\n",
    "\n",
    "        # convenience: write a file with the column names. \n",
    "        # with open(DATASET_FOLDER / f'columns_for_{region}.json', 'w') as f:\n",
    "        #     f.write(json.dumps(sorted(list(set(dataset.columns) - {'day', 'is_augment_row'})), indent=2))\n",
    "                \n",
    "    df = dataset.copy()\n",
    "\n",
    "    # ignore some columns becuase it seems to improve accuracy (and faster training)\n",
    "    ignore_cont = ['day', 'is_augment_row'] +  ignored_columns(df.columns, target, lag)\n",
    "    \n",
    "    # Select columns\n",
    "    if switch == 'full_training' or switch == 'full_training_with_holdout': \n",
    "        # add another column for our target forecast\n",
    "        base_col = target.split('_Tp')[0]  # Either {region}_Price or {region}_Greenness\n",
    "        df[target] = df[base_col].shift(-12 * lag)\n",
    "        df = df.dropna()\n",
    "        categorical = [] # ['month', 'hour', 'weekday']\n",
    "        y_names = [target]\n",
    "        continuous = sorted(list(set(df.columns) - set(categorical) - set(y_names) - set(ignore_cont)))\n",
    "\n",
    "    elif switch == 'greenness_by_fuel': \n",
    "        assert 'Greenness' in target\n",
    "        # add columns for targets - each fuel and total demand in the future. \n",
    "        all_gen = sorted([f'{region}_GEN_{fuel}' for fuel in ['Coal', 'Gas', 'Hydro', 'Rooftop', 'Solar', 'Wind']] + [f'{region}_IC_Fossil_In', f'{region}_IC_Green_In'])\n",
    "        y_names = [f'{feature}_Tp{lag}' for feature in all_gen]\n",
    "                \n",
    "        df[y_names] = df[all_gen].shift(-12 * lag)\n",
    "\n",
    "        df = df.dropna()\n",
    "        categorical = [] # ['month', 'hour', 'weekday']\n",
    "        continuous = sorted(list(set(df.columns) - set(y_names) - set(ignore_cont)))\n",
    "        \n",
    "    elif switch == 'meta_model':\n",
    "        ignore_cont = ['day', 'is_augment_row']   # for metamodel, want ALL forecasts. \n",
    "        # columns for ALL forecast times (lag input parameter above is ignored)\n",
    "        base_col = target.split('_Tp')[0]  # Either {region}_Price or {region}_Greenness\n",
    "        lags = pd.DataFrame(index=df.index)\n",
    "        for lag in FORECAST_TIMES:\n",
    "            lags[f'{base_col}_Tp{lag}'] = df[base_col].shift(-12 * lag)\n",
    "        df = pd.concat([df, lags], axis=1)\n",
    "        df = df.dropna()\n",
    "        categorical = []  # ['month', 'hour', 'weekday']\n",
    "        y_names = [f'{base_col}_Tp{lag}' for lag in FORECAST_TIMES]\n",
    "        continuous = sorted(list(set(df.columns) - set(categorical) - set(y_names) - set(ignore_cont)))\n",
    "                \n",
    "        \n",
    "    # validation_type = 'cross-validation'\n",
    "    # validation_type = 'exclude holdout'\n",
    "    # if validation_set == -1: validation_type = 'holdout set'\n",
    "    if switch == 'full_training_with_holdout':\n",
    "        validation_type = '1month with holdout'\n",
    "    else: \n",
    "        validation_type = 'cross-validation'  # default\n",
    "    \n",
    "    if validation_type == '2020+':\n",
    "        df['validation_set'] = df.index.year >= 2020 \n",
    "    elif validation_type == 'cross-validation':\n",
    "        # we want 1 month in every 5 (20%) for validation. \n",
    "        # also months mod 5 conveniently causes the two to rotat each year which month they are.\n",
    "        df['validation_set'] = (df.index.year*12 + df.index.month) % 5\n",
    "        df['is_validation_set'] = (df.validation_set == validation_set)\n",
    "    elif validation_type == '1month with holdout':\n",
    "        # 1 month in every 5 AND we want the final two-ish months to be a holdout set - exclude these\n",
    "        df['validation_set'] = (df.index.year*12 + df.index.month) % 5\n",
    "        df['is_validation_set'] = (df.validation_set == validation_set)\n",
    "        # if switch == 'meta_model':\n",
    "        #     df = df[(df.validation_set == validation_set) | (df.index >= HOLDOUT_SET_START)]\n",
    "        #     df = df[df.is_augment_row == False]  # remove all augment data for meta_model\n",
    "        #     validation_set = -1  # so we keep the holdoutset below\n",
    "        if validation_set != -1:  # holdout the last two months (but if validation_set = -1, we still want those two months)\n",
    "            df = df[df.index < HOLDOUT_SET_START]\n",
    "        else:  # validation_set == -1, ie we actually want the holdout set.\n",
    "            df['is_validation_set'] = (df.index >= HOLDOUT_SET_START)\n",
    "    \n",
    "    if 'Price' in target:\n",
    "        # remove any augment data in the validation set\n",
    "        df = df[~(df.is_validation_set & df.is_augment_row)]\n",
    "    else:\n",
    "        # remove all augment data for greenness, it's only for price\n",
    "        df = df[df.is_augment_row == False]\n",
    "        \n",
    "    # make sure df's columns are in alphabetical order\n",
    "    df = df.sort_index(axis=1)\n",
    "    \n",
    "    return df, sorted(y_names), sorted(continuous), sorted(categorical)  \n",
    "\n",
    "#test:\n",
    "# df, y_names, continuous, categorical = read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y09EwRBxEcKM",
    "tags": []
   },
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z54O4MIxVY4U",
    "tags": []
   },
   "source": [
    "## FastAI NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 183725,
     "status": "ok",
     "timestamp": 1654958822293,
     "user": {
      "displayName": "Matt Yeung",
      "userId": "15328585613911183133"
     },
     "user_tz": -600
    },
    "id": "PsH7WRGmS67x",
    "outputId": "f35c68a4-5dc5-47ae-fad0-cb05cc0349f3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fastai_nn_prep(df, y_names, continuous, categorical):\n",
    "    splits = ColSplitter('is_validation_set')(df)\n",
    "\n",
    "    to = TabularPandas(df, \n",
    "                       y_names = y_names, \n",
    "                       cat_names = categorical,\n",
    "                       cont_names = continuous, \n",
    "                       procs = [Normalize], # used to also have Categorify, FillMissing, \n",
    "                       splits = splits)\n",
    "    dls = to.dataloaders(bs=128)\n",
    "    \n",
    "    return dls\n",
    "# dls.show_batch()\n",
    "\n",
    "def fastai_nn_train(dls, y_names, epochs, hypers, run_id):\n",
    "    print(f\"=================  Fastai NN Fitting  {y_names[0]}\")\n",
    "    if 'Price' in y_names[0]:\n",
    "        y_range = (p_min, p_max)\n",
    "    elif 'Greenness' in y_names[0]:\n",
    "        y_range = (0,100)\n",
    "    else:\n",
    "        y_range = None\n",
    "    \n",
    "    print(f\"\\n\\n{run_id}: {hypers['layers']}, lr={hypers['lr']}\")\n",
    "    learner = tabular_learner(dls,\n",
    "                            layers=hypers['layers'],\n",
    "                            # metrics=[rmse, mae],\n",
    "                            lr=hypers['lr'], #0.003 best - prev #default=0.001, 0.0005 was good\n",
    "                            loss_func=L1LossFlat(), #MSE if this is commented out\n",
    "                            y_range=y_range,\n",
    "                            config=tabular_config(ps=0.1,embed_p=0.1),\n",
    "                            ) \n",
    "\n",
    "    learner.fit_one_cycle(n_epoch=epochs,\n",
    "                        lr_max=hypers['lr'],\n",
    "                        cbs=[\n",
    "                            # TrackerCallback(monitor='valid_loss'),\n",
    "                            EarlyStoppingCallback(patience=7),\n",
    "                            SaveModelCallback(fname=f\"nn_{run_id}\"),\n",
    "                            CSVLogger(fname=f\"models//history_{run_id}.csv\"),\n",
    "                        ],\n",
    "                       )\n",
    "    return learner\n",
    "\n",
    "learn = None  # declaring global so we can inspect later\n",
    "\n",
    "\n",
    "# save_naked_fastai_nn() saves the model but without any callbacks to reduce filesize.\n",
    "# Note that the recorder callback is the one that takes all the size but removing just that seemed to cause errors.\n",
    "def save_naked_fastai_nn(learn, run_id='tmp'):\n",
    "    while len(learn.cbs): # for some reason we need to do this more than once. \n",
    "        learn.remove_cbs(learn.cbs)\n",
    "    # print(learn.cbs)\n",
    "    filename = f'models//{run_id}.pkl'\n",
    "    learn.export(filename)\n",
    "    print(f'Saved model at {filename}, {os.path.getsize(filename) / 1000000.0 : 0.2f}MB')\n",
    "    return filename\n",
    "\n",
    "def load_fastai(run_id):\n",
    "    return load_learner(f\"models/{run_id}.pkl\")\n",
    "\n",
    "### Fastai NN \n",
    "# train_model() takes in a dict describing the desired model and trains it. \n",
    "# Here's an example model: \n",
    "# model_to_train = {\n",
    "#     'run_id': 'VIC1_Price_Tp84',\n",
    "#     'type': 'fastai_nn',  # 'fastai_nn' or 'xgboost'\n",
    "#     'target': 'VIC1_Price_Tp84',\n",
    "#     'region': 'VIC1',\n",
    "#     'hours_in_future': 84,  # int hours in future to forecast\n",
    "#     'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "#     'layers': [200, 200, 100],  # model structure - found via grid search, specific to this model type\n",
    "#     'lr': 0.001,  # learning rate - found via grid search, specific to this model type\n",
    "#     'switch': 'full_training',  # which dataset variant to use. For dataset8: 'full_training' for price and 'greenness_by_fuel' for greenness\n",
    "#     'validation_set': 0,  # which validation set to use (cross validation)\n",
    "#     'small_filesize': False,\n",
    "# }\n",
    "def train_fastai_nn_model(model_to_train):\n",
    "    region = model_to_train['region']\n",
    "    target = model_to_train['target']\n",
    "    \n",
    "    print(f\"Read dataset {model_to_train['region']}... \", end=\"\")\n",
    "    df, y_names, continuous, categorical = read_dataset(region=model_to_train['region'], \n",
    "                                                        target=model_to_train['target'],\n",
    "                                                        lag=model_to_train['hours_in_future'],\n",
    "                                                        switch=model_to_train['switch'],\n",
    "                                                        validation_set=model_to_train['validation_set'])\n",
    "\n",
    "    print(\"fastai_nn_prep()...\")\n",
    "    global dls  # declare global so can play with it in jupyter. Don't actually rely on this. \n",
    "    dls = fastai_nn_prep(df, y_names, continuous, categorical)\n",
    "\n",
    "    hypers = {'layers': model_to_train['layers'], 'lr': model_to_train['lr']}\n",
    "\n",
    "    global learn  # declare global so can play with it in jupyter. Don't actually rely on this. \n",
    "    learn = fastai_nn_train(dls, y_names, epochs=11, hypers=hypers, run_id=model_to_train['run_id'])\n",
    "\n",
    "    # get results\n",
    "    results = pd.read_csv(f\"models/history_{model_to_train['run_id']}.csv\")['valid_loss']\n",
    "    mae, epochs = results.min(), results.idxmin()\n",
    "    \n",
    "    # save model\n",
    "    if 'small_filesize' in model_to_train and model_to_train['small_filesize'] == True:\n",
    "        filename = save_naked_fastai_nn(learn, model_to_train['run_id'])\n",
    "    else:\n",
    "        # learn.save(f'models/nn{region}')  \n",
    "        filename = f\"models/{model_to_train['run_id']}.pkl\"\n",
    "        learn.export(filename)\n",
    "    \n",
    "    model_to_train['model_filename'] = filename\n",
    "    model_to_train['accuracy (MAE)'] = mae\n",
    "    model_to_train['best_iteration'] = epochs\n",
    "    \n",
    "    return model_to_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NN Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### single runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_to_train = {\n",
    "    'run_id': 'VIC1_Price_Tp84_train_all',\n",
    "    'type': 'fastai_nn',  # 'fastai_nn' or 'xgboost'\n",
    "    'target': 'VIC1_Price_Tp84',\n",
    "    'region': 'VIC1',\n",
    "    'hours_in_future': 84,  # int hours in future to forecast\n",
    "    'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "    'layers': [200, 200, 100],  # model structure - found via grid search, specific to this model type\n",
    "    'lr': 0.001,  # learning rate - found via grid search, specific to this model type\n",
    "    'switch': 'full_training',  # which dataset variant to use\n",
    "    'validation_set': 0,  # which validation set to use (cross validation)\n",
    "    'small_filesize': False,\n",
    "}\n",
    "\n",
    "train_fastai_nn_model(model_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_to_train = {\n",
    "    'run_id': 'QLD1_Greenness_Tp84_fuel',\n",
    "    'type': 'fastai_nn',  # 'fastai_nn' or 'xgboost'\n",
    "    'target': 'QLD1_Greenness_Tp84',\n",
    "    'region': 'QLD1',\n",
    "    'hours_in_future': 84,  # int hours in future to forecast\n",
    "    'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "    'layers': [200, 200, 100],  # model structure - found via grid search, specific to this model type\n",
    "    'lr': 0.0003,  # learning rate - found via grid search, specific to this model type\n",
    "    'switch': 'greenness_by_fuel',  # which dataset variant to use\n",
    "    'validation_set': 1,  # which validation set to use (cross validation)\n",
    "    'small_filesize': False,\n",
    "}\n",
    "\n",
    "train_fastai_nn_model(model_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwbC6yivVmWR",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN XGBOOST\n",
    "###############\n",
    "\n",
    "def new_xgb_model():\n",
    "    return xgboost.XGBRegressor(max_depth=7,  # More is better...  but affects filesize. 12 gave 25mb file and 12.2, 9 gave 5mb file and 12.5. \n",
    "                               n_estimators=300,\n",
    "                               min_child_weight=0.5, #0.5\n",
    "                               colsample_bytree=0.6, #0.6\n",
    "                               subsample=0.8, #0.8\n",
    "                               eta=0.04, # eta is learning rate, 0.01 is best so far\n",
    "                               seed=42,\n",
    "                               eval_metric=[\"mae\"],\n",
    "                               early_stopping_rounds = 14,\n",
    "                               tree_method='gpu_hist', # comment out to use CPU only\n",
    "                               predictor='gpu_predictor', # comment out to use CPU only\n",
    "                               # booster='dart',\n",
    "                               # rate_drop= 0.1,\n",
    "                               # skip_drop=0.5,\n",
    "                              )\n",
    "\n",
    "def train_xgb(x_train, x_valid, y_train, y_valid, plot_importance=False): \n",
    "\n",
    "    xgb = new_xgb_model()\n",
    "    \n",
    "    # verbose = (len(y_names)<=2)\n",
    "    verbose = plot_importance\n",
    "    # verbose = True\n",
    "    xgb.fit(x_train, \n",
    "            y_train, \n",
    "            eval_set=[(x_train, y_train), (x_valid, y_valid)], \n",
    "            verbose=verbose)\n",
    "\n",
    "    # Testing\n",
    "    preds_train_xgb = xgb.predict(x_train, iteration_range=(0, xgb.best_iteration + 1), validate_features=False)\n",
    "    preds_valid_xgb = xgb.predict(x_valid, iteration_range=(0, xgb.best_iteration + 1), validate_features=False)\n",
    "\n",
    "    return preds_train_xgb, preds_valid_xgb, xgb\n",
    "\n",
    "\n",
    "def xgb_prep(df, continuous, target):\n",
    "    \n",
    "    validation = df[df['is_validation_set'] == True]\n",
    "    training = df[df['is_validation_set'] == False]\n",
    "    \n",
    "    xgb_column_names = continuous\n",
    "    \n",
    "    x_train = training[xgb_column_names].to_numpy(dtype='float32')\n",
    "    x_valid = validation[xgb_column_names].to_numpy(dtype='float32')\n",
    "\n",
    "    y_train = training[target]\n",
    "    y_valid = validation[target]\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "    \n",
    "    \n",
    "### train_xgb_model(model_to_train) - pass in this config dict:\n",
    "# model_to_train = {\n",
    "#     'run_id': f'VIC1_Price_Tp84_xbg_temp',\n",
    "#     'type': 'xgboost',  # 'fastai_nn' or 'xgboost'\n",
    "#     'target': 'VIC1_Price_Tp84',\n",
    "#     'region': 'VIC1',\n",
    "#     'hours_in_future': 84,  # int hours in future to forecast\n",
    "#     'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "#     'switch': 'full_training_with_holdout',  # which dataset variant to use\n",
    "#     'validation_set': 0,  # which validation set to use (cross validation)\n",
    "#     'plot_importance': True,\n",
    "# }\n",
    "def train_xgb_model(model_to_train):\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    region = model_to_train['region']\n",
    "    target = model_to_train['target']\n",
    "    \n",
    "    print(f\"Read dataset {model_to_train['region']}... \", end=\"\")\n",
    "    df, y_names, continuous, categorical = read_dataset(region=region, \n",
    "                                                        target=target,\n",
    "                                                        lag=model_to_train['hours_in_future'],\n",
    "                                                        switch=model_to_train['switch'],\n",
    "                                                        validation_set=model_to_train['validation_set'])\n",
    "    print(\"xgb_prep()...\")\n",
    "    x_train, x_valid, y_train, y_valid = xgb_prep(df, continuous, target)\n",
    "\n",
    "    \n",
    "    # plot_importance = (target == f'{region}_Price_Tp84')\n",
    "    if 'plot_importance' in model_to_train:\n",
    "        plot_importance = True\n",
    "    else:\n",
    "        plot_importance = False\n",
    "\n",
    "    print(f\"=================  XGBoost Fitting {model_to_train['run_id']}     {datetime.now()}\")\n",
    "    preds_train_xgb, preds_valid_xgb, xgb = train_xgb(x_train, x_valid, y_train, y_valid, plot_importance)\n",
    "\n",
    "    \n",
    "    if plot_importance:\n",
    "        # set column names for plot_importance\n",
    "        xgb.get_booster().feature_names = continuous\n",
    "        \n",
    "        old = plt.rcParams[\"figure.figsize\"]\n",
    "        plt.rcParams[\"figure.figsize\"] = (16,25)\n",
    "        xgboost.plot_importance(xgb)\n",
    "        plt.rcParams[\"figure.figsize\"] = old\n",
    "\n",
    "    \n",
    "    filename = f\"models/{model_to_train['run_id']}.txt\"\n",
    "    xgb.save_model(filename)\n",
    "\n",
    "    model_to_train['model_filename'] = filename\n",
    "    model_to_train['accuracy (MAE)'] = xgb.evals_result()['validation_1']['mae'][xgb.best_iteration]\n",
    "    model_to_train['best_iteration'] = xgb.best_iteration\n",
    "\n",
    "    print(f\"Time: {timeit.default_timer() - start_time:0.1f}s\")\n",
    "    return model_to_train #, xgb, preds_valid_xgb\n",
    "    \n",
    "\n",
    "# predictions with XGB\n",
    "def load_xgb_model(filename):\n",
    "    xgb = new_xgb_model()\n",
    "    xgb.load_model(filename)\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#xgb - train one model\n",
    "\n",
    "model_to_train = {\n",
    "    'run_id': f'VIC1_Price_Tp84_xbg_temp',\n",
    "    'type': 'xgboost',  # 'fastai_nn' or 'xgboost'\n",
    "    'target': 'VIC1_Price_Tp84',\n",
    "    'region': 'VIC1',\n",
    "    'hours_in_future': 84,  # int hours in future to forecast\n",
    "    'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "    'switch': 'full_training',  # which dataset variant to use\n",
    "    'validation_set': 2,  # which validation set to use (cross validation)\n",
    "    'plot_importance': True,\n",
    "}\n",
    "\n",
    "train_xgb_model(model_to_train)\n",
    "\n",
    "print(f\"{os.path.getsize(f'models/VIC1_Price_Tp84_xbg_temp.txt') / 1000000:0.2f} MB (baseline = 9.5MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGB test 1 model against holdout\n",
    "\n",
    "forecast = 84\n",
    "\n",
    "region = 'VIC1'\n",
    "target = f\"{region}_Price_Tp{forecast}\"\n",
    "print(f\"Read dataset {model_to_train['region']}... \", end=\"\")\n",
    "df, y_names, continuous, categorical = read_dataset(region=region, \n",
    "                                                    target=target,\n",
    "                                                    lag=forecast,\n",
    "                                                    switch=model_to_train['switch'],\n",
    "                                                    validation_set=-1)\n",
    "\n",
    "x_valid = df[df.is_validation_set == True][continuous].to_numpy(dtype='float32')\n",
    "y_valid = df[df.is_validation_set == True][target]\n",
    "\n",
    "filename = f'models\\\\{model_to_train[\"run_id\"]}.txt'\n",
    "\n",
    "xgb = load_xgb_model(filename)\n",
    "\n",
    "preds_valid_xgb = xgb.predict(x_valid, iteration_range=(0, xgb.best_ntree_limit), validate_features=False)\n",
    "\n",
    "mae = mean_absolute_error(y_valid, preds_valid_xgb)\n",
    "print(f\"MAE on holdout dataset: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGB Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Grid Search XGB\n",
    "###################\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, LeaveOneGroupOut\n",
    "\n",
    "# Manually add one-hot encodings for categorical variables\n",
    "enc = preprocessing.OneHotEncoder(sparse=False, dtype='float32') # sparse = false: will return an np array\n",
    "enc.fit(training[categorical])\n",
    "\n",
    "def one_hot_encode_categoricals(df, continuous, categorical):\n",
    "    array = df[continuous].to_numpy(dtype='float32')\n",
    "    return np.append(array, enc.transform(df[categorical]), axis=1)\n",
    "    \n",
    "# cross-validation: no separate test/validation set. all goes in x_train for now.\n",
    "x_train = one_hot_encode_categoricals(df, continuous, categorical)\n",
    "\n",
    "y_train = df[y_names[0]]\n",
    "\n",
    "xgb = xgboost.XGBRegressor(#n_estimators=40,\n",
    "                           colsample_bytree=0.6, # colsample_bytree=0.8, \n",
    "                           # max_depth=5, # max_depth=10,\n",
    "                           min_child_weight=0.5, \n",
    "                           subsample=0.8, \n",
    "                           eta=0.1, # eta is learning rate\n",
    "                           seed=42,\n",
    "                           n_jobs=1,\n",
    "                           eval_metric=[\"mae\"],\n",
    "                           # early_stopping_rounds = 14,\n",
    "                           tree_method='gpu_hist', # comment out to use CPU only\n",
    "                           predictor='gpu_predictor', # comment out to use CPU only\n",
    "                           )\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [40, 50, 60, 70],\n",
    "    'eta' : [0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "    # 'colsample_bytree': [0.5, 0.6, 0.7, 0.8],\n",
    "    'max_depth': [3, 4, 5, 8], \n",
    "    # 'min_child_weight': [0.5], # doens't make a difference 0.3 ... 0.8\n",
    "    # 'subsample': [0.7, 0.8, 0.9], # 0.8 betas 0.6, 1.0 by a little bit\n",
    "    # 'gamma': [0],\n",
    "}\n",
    "\n",
    "# 5 batches, each with 6 months of validation. the first one will be the first 8years of data, 2nd is first 8.5, etc. \n",
    "cross_validation = TimeSeriesSplit(n_splits=5, gap=12*24*1, test_size=12*24*180) # 6 month splits. \n",
    "\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring='neg_mean_absolute_error', cv=cross_validation, \n",
    "                           n_jobs=-1, verbose=1, return_train_score=True)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_search_results.to_csv('grid_search_results2.csv')\n",
    "print(\"results\")\n",
    "grid_search_results.sort_values('rank_test_score')[[x for x in grid_search_results.columns if 'train' not in x and 'params' not in x]][:40]\n",
    "\n",
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Batching Many Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_to_train):\n",
    "    if model_to_train['type'] == 'fastai_nn':\n",
    "        return train_fastai_nn_model(model_to_train)\n",
    "    elif model_to_train['type'] == 'xgboost':\n",
    "        return train_xgb_model(model_to_train)\n",
    "    else:\n",
    "        assert False, 'invalid model type requested'\n",
    "        \n",
    "def print_queue():\n",
    "    with open('models_to_train.json') as f:\n",
    "        return pd.DataFrame(json.loads(f.read()))\n",
    "\n",
    "def save_queue(models_to_train):\n",
    "    out = json.dumps(models_to_train)\n",
    "    with open('models_to_train.json', 'w') as f:\n",
    "        f.write(out)\n",
    "\n",
    "def load_queue():\n",
    "    for _ in range(5):\n",
    "        with open('models_to_train.json') as f:\n",
    "            raw = f.read()\n",
    "        if len(raw) > 100:\n",
    "            break\n",
    "    else:\n",
    "        assert False, \"models_to_train.json came back < 100 bytes\"\n",
    "    return json.loads(raw)\n",
    "\n",
    "def update_queue(run_id, key, value):\n",
    "    models = load_queue()\n",
    "    for i, model in enumerate(models):\n",
    "        if model['run_id'] == run_id:\n",
    "            models[i][key] = value\n",
    "            break\n",
    "    save_queue(models)\n",
    "    return models\n",
    "\n",
    "def reset_pending_in_queue():\n",
    "    print('These are marked \"pending\":')\n",
    "    models = load_queue()\n",
    "    for model in models:\n",
    "        if model['model_filename'] == 'pending':\n",
    "            print(model['run_id'])\n",
    "            model['model_filename'] = None\n",
    "    print('... not any more.')\n",
    "    save_queue(models)\n",
    "    return load_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train the next thing on the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train lots of models\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    # refresh the queue from disk and find the next one to train\n",
    "    models_to_train = load_queue()\n",
    "    if i >= len(models_to_train):\n",
    "        break\n",
    "    model_to_train = models_to_train[i]\n",
    "    i += 1\n",
    "\n",
    "    # skip if already trained / pending\n",
    "    if model_to_train['model_filename'] is not None: continue\n",
    "    \n",
    "    # if model_to_train['type'] != 'fastai_nn': continue  # skip NNs, xgboost only \n",
    "    # if model_to_train['region'] != 'VIC1': continue  # Only a specific region\n",
    "    \n",
    "\n",
    "    models_to_train = update_queue(model_to_train['run_id'], 'model_filename', 'pending')\n",
    "    \n",
    "    results = train(model_to_train)\n",
    "    \n",
    "    models_to_train = update_queue(model_to_train['run_id'], 'best_iteration', results['best_iteration'])\n",
    "    models_to_train = update_queue(model_to_train['run_id'], 'accuracy (MAE)', results['accuracy (MAE)'])\n",
    "    models_to_train = update_queue(model_to_train['run_id'], 'model_filename', results['model_filename'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build the queue for training EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build list of models to train\n",
    "if os.path.exists('models_to_train.json'):\n",
    "    print('models_to_train.json already exists, not overwriting')\n",
    "    models_to_train = load_queue()\n",
    "else:\n",
    "    # create a fresh models_to_train list\n",
    "    print(f\"Forecasts for: {FORECAST_TIMES}\")\n",
    "\n",
    "    models_to_train = []\n",
    "    for model_type in ['fastai_nn', 'xgboost']:\n",
    "        for region in REGIONIDS:\n",
    "            for price_or_greenness in ['Price', 'Greenness']:\n",
    "                for forecast_time in FORECAST_TIMES:\n",
    "                    short_model_type = \"nn\" if model_type == \"fastai_nn\" else \"xgb\"\n",
    "                    switch = 'full_training' if price_or_greenness == 'Price' else 'greenness_by_fuel'\n",
    "                    if price_or_greenness == 'Greenness' and model_type == 'xgboost': continue\n",
    "                    model_to_train = {\n",
    "                        'run_id': f\"{region}_{price_or_greenness}_Tp{forecast_time}_{short_model_type}\",\n",
    "                        'type': model_type,  # 'fastai_nn' or 'xgboost'\n",
    "                        'target': f\"{region}_{price_or_greenness}_Tp{forecast_time}\",  # eg VIC1_Price_Tp84 or NSW1_Greenness_Tp1\n",
    "                        'region': region,\n",
    "                        'hours_in_future': forecast_time,  # int hours in future to forecast\n",
    "                        'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "                        'layers': [200, 200, 100],  # found via grid search, specific to this model type\n",
    "                        'lr': 0.0003,  # found via grid search, specific to this model type\n",
    "                        'switch': switch,  # which dataset variant to use\n",
    "                        'validation_set': 1,  # which validation set to use (cross validation)\n",
    "                        'small_filesize': True,\n",
    "                    }\n",
    "                    models_to_train.append(model_to_train)\n",
    "\n",
    "    save_queue(models_to_train)\n",
    "\n",
    "print(f\"Goal: Train {len(models_to_train)} models\")\n",
    "print_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NN Gridsearch - build a queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Fastai NN Gridsearch\n",
    "# Deletes current queue file models_to_train.json !\n",
    "\n",
    "\n",
    "\n",
    "layers_to_try = [[400,400,200],\n",
    "                 [200,100],\n",
    "                 [300,100],\n",
    "                 [300,200]\n",
    "                ]\n",
    "# lr = [0.1, 0.03, 0.01, 0.003]\n",
    "# lr = [0.03, 0.01, 0.003, 0.001]\n",
    "# lrs = [0.003, 0.001, 0.0003]\n",
    "# lrs = [0.001]\n",
    "lrs = [0.0003]\n",
    "\n",
    "models_to_train = []\n",
    "for region in ['QLD1']: # REGIONIDS:  # in ['VIC1']\n",
    "    for forecast in [84]: #[4, 48, 84, 162]:\n",
    "        for layers in layers_to_try:\n",
    "            for lr in lrs: #[0.003, 0.001, 0.0003]:\n",
    "                # learn = fastai_nn_train(dls, y_names, epochs=18, hypers=hypers, run_id=f'Gridsearch_{i}')\n",
    "                model_to_train = {\n",
    "                    'run_id':  f'Gridsearch_{len(models_to_train)}',\n",
    "                    'type': 'fastai_nn',  # 'fastai_nn' or 'xgboost'\n",
    "                    'target': f'{region}_Greenness_Tp{forecast}',\n",
    "                    'region': region,\n",
    "                    'hours_in_future': forecast,  # int hours in future to forecast\n",
    "                    'model_filename': None,  # initialised to None, contains filename of model after training\n",
    "                    'layers': layers,  # model structure - found via grid search, specific to this model type\n",
    "                    'lr': lr,  # learning rate - found via grid search, specific to this model type\n",
    "                    'switch': 'greenness_by_fuel',  # which dataset variant to use\n",
    "                    'validation_set': 0,  # which validation set to use (cross validation)\n",
    "                    'small_filesize': False,\n",
    "                }\n",
    "\n",
    "                # train_fastai_nn_model(model_to_train)\n",
    "                models_to_train.append(model_to_train)\n",
    "\n",
    "#     mae = pd.read_csv(f\"models//history_{run_id}.csv\")['valid_loss'].min()\n",
    "#     # mae = learn.tracker.best\n",
    "#     print(f\"Best = {mae}\")\n",
    "#     hyperparams[i]['Result (Valid_MAE)'] = mae\n",
    "\n",
    "#     hyperparams_df = pd.DataFrame(hyperparams)\n",
    "#     hyperparams_df.to_csv(f'results_{region}.csv')\n",
    "    \n",
    "# hyperparams_df\n",
    "print(f'Gridsearch Queue: {len(models_to_train)} combinations')\n",
    "\n",
    "save_queue(models_to_train)\n",
    "print_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Spijf7QEWzK",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds_train_xgb \n",
    "# preds_valid_xgb \n",
    "df2 = validation.copy()\n",
    "df3 = training.copy()\n",
    "\n",
    "if \"preds_valid_xgb\" in locals():\n",
    "    df2['preds_xgb'] = preds_valid_xgb\n",
    "    df3['preds_xgb'] = preds_train_xgb\n",
    "if \"preds_valid\" in locals():\n",
    "    df2['preds_nn'] = preds_valid\n",
    "    df3['preds_nn'] = preds_train\n",
    "if \"preds_valid_ens\" in locals():\n",
    "    df2['preds_ens'] = preds_valid_ens\n",
    "    df3['preds_ens'] = preds_train_ens\n",
    "    \n",
    "df2 = pd.concat([df2, df3]).sort_index()\n",
    "df3 = 0\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols=[target_col]\n",
    "if \"preds_valid_xgb\" in locals():\n",
    "    plot_cols.append('preds_xgb')\n",
    "if \"preds_valid\" in locals():\n",
    "    plot_cols.append('preds_nn')\n",
    "if \"preds_valid_ens\" in locals():\n",
    "    plot_cols.append('preds_ens')\n",
    "    \n",
    "def plot_month(df, date, columns=plot_cols, days=32):\n",
    "    if isinstance(date, str):\n",
    "        date = pd.to_datetime(date)\n",
    "    start = date\n",
    "    end = date + pd.Timedelta(days=days)\n",
    "    df[start:end].plot(y=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_month(df2, '2022-5-1', days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_month(df2, '2022-3-1', days=10)\n",
    "plot_month(df2, '2022-3-10', days=10)\n",
    "plot_month(df2, '2022-3-20', days=10)\n",
    "plot_month(df2, '2022-4-1', days=10)\n",
    "plot_month(df2, '2022-4-10', days=10)\n",
    "plot_month(df2, '2022-4-20', days=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Spijf7QEWzK",
    "tags": []
   },
   "source": [
    "## Plot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "start = 5200\n",
    "x=240\n",
    "\n",
    "# offset=val_start\n",
    "offset=0\n",
    "\n",
    "plt.plot(range(x), preds_train_rf[start:start+x], 'r.-', \n",
    "         range(x), targs_train_rf[start:start+x], 'b.-',\n",
    "         range(x), decode_price(df[f'{region}_Price_Tm1'][offset+start:offset+start+x]), 'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1654963178586,
     "user": {
      "displayName": "Matt Yeung",
      "userId": "15328585613911183133"
     },
     "user_tz": -600
    },
    "id": "lpAfz613S2oZ",
    "tags": []
   },
   "source": [
    "## Plot Greenness errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = validation.copy()\n",
    "val_df['preds_xgb'] = preds_valid_xgb\n",
    "val_df['xgb_error'] = val_df[target_col] - val_df['preds_xgb']\n",
    "worst_days = val_df['xgb_error'].abs().resample('D').mean().sort_values(ascending=False).index[:10]\n",
    "worst_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_renewables(df, date):\n",
    "    start = date - pd.Timedelta(days=3)\n",
    "    end  =  date + pd.Timedelta(days=2)\n",
    "    df.loc[start:end].plot.area(\n",
    "        y=['VIC1_GEN_Wind', 'VIC1_GEN_Hydro', 'VIC1_GEN_Solar', 'VIC1_GEN_Rooftop'],\n",
    "        color=['#2dc3fa', '#075cfa', '#fae22d', '#fca128'],\n",
    "        title=f\"Renewables for dates around {date.date()}\")\n",
    "plot_renewables(df, pd.to_datetime('2020-02-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_day = 3\n",
    "def plot_day(df, date):\n",
    "    start = date - pd.Timedelta(days=3)\n",
    "    end  =  date + pd.Timedelta(days=2)\n",
    "    df.loc[start:end].plot(\n",
    "        y=[target_col,'preds_xgb', 'xgb_error'], \n",
    "        color=['black','blue','r'], \n",
    "        title=f\"plot_worst_day() - Errors for {date.date()} and nearby days\",\n",
    "        grid=True)\n",
    "    # return df.loc[start:end]\n",
    "\n",
    "plot_day(a, worst_days[which_day])\n",
    "plot_renewables(a, worst_days[which_day])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_month(df4, '2020-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "executionInfo": {
     "elapsed": 1203,
     "status": "ok",
     "timestamp": 1654964628136,
     "user": {
      "displayName": "Matt Yeung",
      "userId": "15328585613911183133"
     },
     "user_tz": -600
    },
    "id": "3ajZMNV-PUFR",
    "outputId": "6b81cea5-de60-497c-9afc-18a744043931"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,16)\n",
    "fig = plt.plot(targs_valid, preds_valid, 'b.', [-200,600], [-200,600], '-')\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([0,200])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsHWk6g43F9mhd4tKan8xe",
   "name": "ElecForecast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
